{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.5.1\n",
      "IPython 5.1.0\n",
      "\n",
      "numpy 1.11.1\n",
      "seaborn 0.7.1\n",
      "matplotlib 1.5.1\n",
      "pandas 0.18.1\n",
      "theano 0.8.2\n",
      "tensorflow 0.10.0rc0\n",
      "keras 1.0.7\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -p numpy,seaborn,matplotlib,pandas,theano,tensorflow,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "from pylab import rcParams\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Spectral', font_scale=1.5)\n",
    "\n",
    "material_palette = [\"#4CAF50\", \"#2196F3\", \"#9E9E9E\", \"#FF9800\", \"#607D8B\", \"#9C27B0\"]\n",
    "sns.set_palette(material_palette)\n",
    "rcParams['figure.figsize'] = 16, 8\n",
    "\n",
    "plt.xkcd();\n",
    "random_state = 42\n",
    "np.random.seed(random_state);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = \"data/mnist.pkl.gz\"\n",
    "\n",
    "with gzip.open(dataset, 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "train_x, train_y = train_set\n",
    "valid_x, valid_y = valid_set\n",
    "test_x, test_y = test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28 # MNIST data input (img shape: 28*28)\n",
    "hidden_units = 512\n",
    "output_size = 10 # MNIST data classes (digits 0-9)\n",
    "batch_size = 128\n",
    "train_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_shared = theano.shared(value=np.asarray(train_x, dtype='float32'), name='train_x')\n",
    "train_y_shared = theano.shared(value=np.asarray(train_y, dtype='int32'), name='train_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    \n",
    "    def __init__(self, data, input_size, output_size, weights=None, biases=None, activation=T.tanh):\n",
    "        self.data = data\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation=activation\n",
    "        \n",
    "        if weights is None:\n",
    "            w_values = np.asarray(np.random.uniform(\n",
    "                    low=-np.sqrt(6. / (input_size + output_size)),\n",
    "                    high=np.sqrt(6. / (input_size + output_size)),\n",
    "                    size=(input_size, output_size)), dtype=theano.config.floatX)\n",
    "            if activation == theano.tensor.nnet.sigmoid or activation == theano.tensor.nnet.hard_sigmoid or activation == theano.tensor.nnet.ultra_fast_sigmoid:\n",
    "                w_values *= 4\n",
    "\n",
    "            weights = theano.shared(value=w_values, name='weights')\n",
    "\n",
    "        if biases is None:\n",
    "            b_values = np.zeros((output_size,), dtype=theano.config.floatX)\n",
    "            biases = theano.shared(value=b_values, name='biases')\n",
    "\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "        self.output = T.dot(data, self.weights) + self.biases\n",
    "        if activation is not None:\n",
    "            self.output = activation(self.output)\n",
    "        \n",
    "        self.params = [self.weights, self.biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    \n",
    "    def __init__(self, data, input_size, output_size):\n",
    "        self.weights = theano.shared(value=np.zeros((input_size, output_size), \n",
    "                                                dtype=theano.config.floatX), name='weights')\n",
    "        self.biases = theano.shared(value=np.zeros((output_size,),\n",
    "                                                dtype=theano.config.floatX), name='biases')\n",
    "        \n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(data, self.weights) + self.biases)\n",
    "        \n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "     def __init__(self, data, input_size, hidden_layer_size, output_size):\n",
    "        self.hidden_layer = HiddenLayer(\n",
    "            data, input_size=input_size, output_size=hidden_layer_size,\n",
    "            activation=T.tanh)\n",
    "        \n",
    "        self.softmax_layer = SoftmaxLayer(\n",
    "            data=self.hidden_layer.output,\n",
    "            input_size=hidden_layer_size, output_size=output_size)\n",
    "        \n",
    "        self.negative_log_likelihood = self.softmax_layer.negative_log_likelihood\n",
    "        \n",
    "        self.params = self.hidden_layer.params + self.softmax_layer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "index = T.lscalar()\n",
    "x = T.fmatrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "classifier = MLP(data=x, input_size=input_size, hidden_layer_size=hidden_units, output_size=output_size)\n",
    "\n",
    "cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "updates = [(param, param - learning_rate * T.grad(cost, param) ) for param in classifier.params]\n",
    "\n",
    "train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_x_shared[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_y_shared[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "predict_labels = theano.function(inputs=[x], outputs=classifier.softmax_layer.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: Accurarcy on validation: 0.840600, accurarcy on test: 0.830900\n",
      "epoch 2: Accurarcy on validation: 0.872900, accurarcy on test: 0.866800\n",
      "epoch 3: Accurarcy on validation: 0.886700, accurarcy on test: 0.881400\n",
      "epoch 4: Accurarcy on validation: 0.894000, accurarcy on test: 0.889500\n",
      "epoch 5: Accurarcy on validation: 0.898400, accurarcy on test: 0.894500\n",
      "epoch 6: Accurarcy on validation: 0.901900, accurarcy on test: 0.898500\n",
      "epoch 7: Accurarcy on validation: 0.905200, accurarcy on test: 0.900700\n",
      "epoch 8: Accurarcy on validation: 0.907100, accurarcy on test: 0.902200\n",
      "epoch 9: Accurarcy on validation: 0.909600, accurarcy on test: 0.906200\n",
      "epoch 10: Accurarcy on validation: 0.911400, accurarcy on test: 0.907200\n",
      "epoch 11: Accurarcy on validation: 0.912300, accurarcy on test: 0.909300\n",
      "epoch 12: Accurarcy on validation: 0.913900, accurarcy on test: 0.911200\n",
      "epoch 13: Accurarcy on validation: 0.915000, accurarcy on test: 0.913000\n",
      "epoch 14: Accurarcy on validation: 0.915700, accurarcy on test: 0.914400\n",
      "epoch 15: Accurarcy on validation: 0.916700, accurarcy on test: 0.916000\n",
      "epoch 16: Accurarcy on validation: 0.917900, accurarcy on test: 0.916700\n",
      "epoch 17: Accurarcy on validation: 0.918300, accurarcy on test: 0.917200\n",
      "epoch 18: Accurarcy on validation: 0.919300, accurarcy on test: 0.918100\n",
      "epoch 19: Accurarcy on validation: 0.919800, accurarcy on test: 0.919400\n",
      "epoch 20: Accurarcy on validation: 0.920700, accurarcy on test: 0.920000\n"
     ]
    }
   ],
   "source": [
    "number_of_minibatches = len(train_x) / batch_size\n",
    "\n",
    "def compute_accurarcy(dataset_x, dataset_y): \n",
    "    predictions = predict_labels(dataset_x)\n",
    "    errors = sum(predictions != dataset_y) #Number of errors\n",
    "    accurarcy = 1 - errors/float(len(dataset_y))\n",
    "    return accurarcy\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    for idx in range(0, int(number_of_minibatches)):\n",
    "        train_model(idx)\n",
    "\n",
    "    accurarcy_valid = compute_accurarcy(valid_x, valid_y)\n",
    "    accurarcy_test = compute_accurarcy(test_x, test_y)\n",
    "\n",
    "    print(\"epoch %d: Accurarcy on validation: %f, accurarcy on test: %f\" % (epoch + 1, accurarcy_valid, accurarcy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist_tf/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist_tf/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist_tf/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist_tf/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets('data/mnist_tf', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1171875\n",
      "step 1000, training accuracy 0.890625\n",
      "step 2000, training accuracy 0.921875\n",
      "step 3000, training accuracy 0.96875\n",
      "step 4000, training accuracy 0.9921875\n",
      "step 5000, training accuracy 0.9765625\n",
      "step 6000, training accuracy 0.9765625\n",
      "step 7000, training accuracy 0.9921875\n",
      "step 8000, training accuracy 0.9921875\n",
      "step 9000, training accuracy 1.0\n",
      "step 10000, training accuracy 1.0\n",
      "step 11000, training accuracy 1.0\n",
      "step 12000, training accuracy 1.0\n",
      "step 13000, training accuracy 1.0\n",
      "step 14000, training accuracy 1.0\n",
      "step 15000, training accuracy 1.0\n",
      "step 16000, training accuracy 1.0\n",
      "step 17000, training accuracy 1.0\n",
      "step 18000, training accuracy 1.0\n",
      "step 19000, training accuracy 1.0\n",
      "test accuracy 0.9575999975204468\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "n_hidden_1 = int(hidden_units / 2)\n",
    "n_hidden_2 = int(hidden_units / 2)\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, input_size])\n",
    "y = tf.placeholder(\"float\", [None, output_size])\n",
    "\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    return tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([input_size, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, output_size]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([output_size]))\n",
    "}\n",
    "\n",
    "model = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(model, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(model, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as s:\n",
    "    s.run(tf.initialize_all_variables())\n",
    "\n",
    "    for i in range(20000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        s.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y})\n",
    "            print('step {0}, training accuracy {1}'.format(i, train_accuracy))\n",
    "    print('test accuracy {0}'.format(accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 512)           401920      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 512)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 512)           262656      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 512)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            5130        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10)            0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 669706\n",
      "____________________________________________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 5s - loss: 0.9675 - acc: 0.6977 - val_loss: 0.4647 - val_acc: 0.8692\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 7s - loss: 0.4098 - acc: 0.8776 - val_loss: 0.3377 - val_acc: 0.8987\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.3221 - acc: 0.9048 - val_loss: 0.2779 - val_acc: 0.9148\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.2655 - acc: 0.9205 - val_loss: 0.2308 - val_acc: 0.9288\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.2226 - acc: 0.9328 - val_loss: 0.1858 - val_acc: 0.9435\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.1901 - acc: 0.9429 - val_loss: 0.1763 - val_acc: 0.9451\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.1656 - acc: 0.9493 - val_loss: 0.1610 - val_acc: 0.9494\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.1453 - acc: 0.9567 - val_loss: 0.1238 - val_acc: 0.9619\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.1301 - acc: 0.9600 - val_loss: 0.1287 - val_acc: 0.9595\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.1174 - acc: 0.9639 - val_loss: 0.1234 - val_acc: 0.9624\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.1071 - acc: 0.9671 - val_loss: 0.1213 - val_acc: 0.9648\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.0992 - acc: 0.9698 - val_loss: 0.1008 - val_acc: 0.9680\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.0917 - acc: 0.9716 - val_loss: 0.0930 - val_acc: 0.9715\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.0857 - acc: 0.9736 - val_loss: 0.1019 - val_acc: 0.9697\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 8s - loss: 0.0791 - acc: 0.9759 - val_loss: 0.0949 - val_acc: 0.9731\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 8s - loss: 0.0758 - acc: 0.9773 - val_loss: 0.0869 - val_acc: 0.9727\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.0701 - acc: 0.9788 - val_loss: 0.0860 - val_acc: 0.9741\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.0667 - acc: 0.9796 - val_loss: 0.0855 - val_acc: 0.9741\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.0634 - acc: 0.9801 - val_loss: 0.0833 - val_acc: 0.9757\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 9s - loss: 0.0595 - acc: 0.9815 - val_loss: 0.0860 - val_acc: 0.9745\n",
      "Test score: 0.0865734727865\n",
      "Test accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "X_train = train_x.astype('float32')\n",
    "X_test = test_x.astype('float32')\n",
    "X_valid = valid_x.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_valid /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(train_y, output_size)\n",
    "Y_test = np_utils.to_categorical(test_y, output_size)\n",
    "Y_valid = np_utils.to_categorical(valid_y, output_size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_units, input_shape=(input_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(hidden_units))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=train_epochs,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_valid, Y_valid, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
